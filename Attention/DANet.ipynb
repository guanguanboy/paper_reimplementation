{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/leftthomas/DANet/blob/master/danet/head.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "import fvcore.nn.weight_init as weight_init\n",
    "import numpy as np\n",
    "import torch\n",
    "from detectron2.layers import Conv2d, ShapeSpec\n",
    "from detectron2.modeling.meta_arch.semantic_seg import SEM_SEG_HEADS_REGISTRY\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "@SEM_SEG_HEADS_REGISTRY.register()\n",
    "class DANetHead(nn.Module):\n",
    "    def __init__(self, cfg, input_shape: Dict[str, ShapeSpec]):\n",
    "        super().__init__()\n",
    "\n",
    "        # fmt: off\n",
    "        self.in_features = cfg.MODEL.SEM_SEG_HEAD.IN_FEATURES\n",
    "        feature_strides = {k: v.stride for k, v in input_shape.items()}\n",
    "        feature_channels = {k: v.channels for k, v in input_shape.items()}\n",
    "        self.ignore_value = cfg.MODEL.SEM_SEG_HEAD.IGNORE_VALUE\n",
    "        num_classes = cfg.MODEL.SEM_SEG_HEAD.NUM_CLASSES\n",
    "        conv_dims = cfg.MODEL.SEM_SEG_HEAD.CONVS_DIM\n",
    "        self.common_stride = cfg.MODEL.SEM_SEG_HEAD.COMMON_STRIDE\n",
    "        norm = cfg.MODEL.SEM_SEG_HEAD.NORM\n",
    "        # fmt: on\n",
    "\n",
    "        self.scale_pam_heads = []\n",
    "        for in_feature in self.in_features:\n",
    "            head_ops = []\n",
    "            head_length = max(\n",
    "                1, int(np.log2(feature_strides[in_feature]) - np.log2(self.common_stride))\n",
    "            )\n",
    "            for k in range(head_length):\n",
    "                norm_module = nn.GroupNorm(32, conv_dims) if norm == \"GN\" else None\n",
    "                conv = Conv2d(\n",
    "                    feature_channels[in_feature] if k == 0 else conv_dims,\n",
    "                    conv_dims,\n",
    "                    kernel_size=3,\n",
    "                    stride=1,\n",
    "                    padding=1,\n",
    "                    bias=not norm,\n",
    "                    norm=norm_module,\n",
    "                    activation=F.relu,\n",
    "                )\n",
    "                weight_init.c2_msra_fill(conv)\n",
    "                head_ops.append(conv)\n",
    "                if feature_strides[in_feature] != self.common_stride:\n",
    "                    head_ops.append(\n",
    "                        nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)\n",
    "                    )\n",
    "            self.scale_pam_heads.append(nn.Sequential(*head_ops))\n",
    "            self.add_module(in_feature + '_pam', self.scale_pam_heads[-1])\n",
    "\n",
    "        self.scale_cam_heads = []\n",
    "        for in_feature in self.in_features:\n",
    "            head_ops = []\n",
    "            head_length = max(\n",
    "                1, int(np.log2(feature_strides[in_feature]) - np.log2(self.common_stride))\n",
    "            )\n",
    "            for k in range(head_length):\n",
    "                norm_module = nn.GroupNorm(32, conv_dims) if norm == \"GN\" else None\n",
    "                conv = Conv2d(\n",
    "                    feature_channels[in_feature] if k == 0 else conv_dims,\n",
    "                    conv_dims,\n",
    "                    kernel_size=3,\n",
    "                    stride=1,\n",
    "                    padding=1,\n",
    "                    bias=not norm,\n",
    "                    norm=norm_module,\n",
    "                    activation=F.relu,\n",
    "                )\n",
    "                weight_init.c2_msra_fill(conv)\n",
    "                head_ops.append(conv)\n",
    "                if feature_strides[in_feature] != self.common_stride:\n",
    "                    head_ops.append(\n",
    "                        nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)\n",
    "                    )\n",
    "            self.scale_cam_heads.append(nn.Sequential(*head_ops))\n",
    "            self.add_module(in_feature + '_cam', self.scale_cam_heads[-1])\n",
    "\n",
    "        self.predictor = Conv2d(conv_dims, num_classes, kernel_size=1, stride=1, padding=0)\n",
    "        weight_init.c2_msra_fill(self.predictor)\n",
    "\n",
    "    def forward(self, features, targets=None):\n",
    "        for i, f in enumerate(self.in_features):\n",
    "            if i == 0:\n",
    "                pam = self.scale_pam_heads[i](features[f])\n",
    "                cam = self.scale_cam_heads[i](features[f])\n",
    "            else:\n",
    "                pam = pam + self.scale_pam_heads[i](features[f])\n",
    "                cam = cam + self.scale_cam_heads[i](features[f])\n",
    "\n",
    "        b, c, h, w = pam.size()\n",
    "        B_T = pam.view(b, c, -1)\n",
    "        B = B_T.transpose(-1, -2).contiguous()\n",
    "        pam_weight = F.softmax(torch.matmul(B, B_T), dim=-1).view(b, 1, h * w, h * w)\n",
    "        weighted_pam = torch.matmul(pam_weight, pam.view(b, c, h * w, 1)).view(b, c, h, w)\n",
    "        sum_pam = pam + weighted_pam\n",
    "\n",
    "        b, c, h, w = cam.size()\n",
    "        A = cam.view(b, c, -1)\n",
    "        A_T = A.transpose(-1, -2).contiguous()\n",
    "        cam_weight = F.softmax(torch.matmul(A, A_T), dim=-1).view(b, 1, c, c)\n",
    "        weighted_cam = torch.matmul(cam_weight, cam.view(b, c, h * w).transpose(-1, -2).contiguous()\n",
    "                                    .view(b, h * w, c, 1)).view(b, h * w, c).transpose(-1, -2).contiguous() \\\n",
    "            .view(b, c, h, w)\n",
    "        sum_cam = cam + weighted_cam\n",
    "\n",
    "        x = self.predictor(sum_pam + sum_cam)\n",
    "        x = F.interpolate(x, scale_factor=self.common_stride, mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "        if self.training:\n",
    "            losses = {}\n",
    "            losses[\"loss_sem_seg\"] = (F.cross_entropy(x, targets, reduction=\"mean\", ignore_index=self.ignore_value))\n",
    "            return [], losses\n",
    "        else:\n",
    "            return x, {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/da_head.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from mmcv.cnn import ConvModule, Scale\n",
    "from torch import nn\n",
    "\n",
    "from mmseg.core import add_prefix\n",
    "from ..builder import HEADS\n",
    "from ..utils import SelfAttentionBlock as _SelfAttentionBlock\n",
    "from .decode_head import BaseDecodeHead\n",
    "\n",
    "\n",
    "class PAM(_SelfAttentionBlock):\n",
    "    \"\"\"Position Attention Module (PAM)\n",
    "    Args:\n",
    "        in_channels (int): Input channels of key/query feature.\n",
    "        channels (int): Output channels of key/query transform.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, channels):\n",
    "        super(PAM, self).__init__(\n",
    "            key_in_channels=in_channels,\n",
    "            query_in_channels=in_channels,\n",
    "            channels=channels,\n",
    "            out_channels=in_channels,\n",
    "            share_key_query=False,\n",
    "            query_downsample=None,\n",
    "            key_downsample=None,\n",
    "            key_query_num_convs=1,\n",
    "            key_query_norm=False,\n",
    "            value_out_num_convs=1,\n",
    "            value_out_norm=False,\n",
    "            matmul_norm=False,\n",
    "            with_out=False,\n",
    "            conv_cfg=None,\n",
    "            norm_cfg=None,\n",
    "            act_cfg=None)\n",
    "\n",
    "        self.gamma = Scale(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward function.\"\"\"\n",
    "        out = super(PAM, self).forward(x, x)\n",
    "\n",
    "        out = self.gamma(out) + x\n",
    "        return out\n",
    "\n",
    "\n",
    "class CAM(nn.Module):\n",
    "    \"\"\"Channel Attention Module (CAM)\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CAM, self).__init__()\n",
    "        self.gamma = Scale(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward function.\"\"\"\n",
    "        batch_size, channels, height, width = x.size()\n",
    "        proj_query = x.view(batch_size, channels, -1)\n",
    "        proj_key = x.view(batch_size, channels, -1).permute(0, 2, 1)\n",
    "        energy = torch.bmm(proj_query, proj_key)\n",
    "        energy_new = torch.max(\n",
    "            energy, -1, keepdim=True)[0].expand_as(energy) - energy\n",
    "        attention = F.softmax(energy_new, dim=-1)\n",
    "        proj_value = x.view(batch_size, channels, -1)\n",
    "\n",
    "        out = torch.bmm(attention, proj_value)\n",
    "        out = out.view(batch_size, channels, height, width)\n",
    "\n",
    "        out = self.gamma(out) + x\n",
    "        return out\n",
    "\n",
    "\n",
    "@HEADS.register_module()\n",
    "class DAHead(BaseDecodeHead):\n",
    "    \"\"\"Dual Attention Network for Scene Segmentation.\n",
    "    This head is the implementation of `DANet\n",
    "    <https://arxiv.org/abs/1809.02983>`_.\n",
    "    Args:\n",
    "        pam_channels (int): The channels of Position Attention Module(PAM).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pam_channels, **kwargs):\n",
    "        super(DAHead, self).__init__(**kwargs)\n",
    "        self.pam_channels = pam_channels\n",
    "        self.pam_in_conv = ConvModule(\n",
    "            self.in_channels,\n",
    "            self.channels,\n",
    "            3,\n",
    "            padding=1,\n",
    "            conv_cfg=self.conv_cfg,\n",
    "            norm_cfg=self.norm_cfg,\n",
    "            act_cfg=self.act_cfg)\n",
    "        self.pam = PAM(self.channels, pam_channels)\n",
    "        self.pam_out_conv = ConvModule(\n",
    "            self.channels,\n",
    "            self.channels,\n",
    "            3,\n",
    "            padding=1,\n",
    "            conv_cfg=self.conv_cfg,\n",
    "            norm_cfg=self.norm_cfg,\n",
    "            act_cfg=self.act_cfg)\n",
    "        self.pam_conv_seg = nn.Conv2d(\n",
    "            self.channels, self.num_classes, kernel_size=1)\n",
    "\n",
    "        self.cam_in_conv = ConvModule(\n",
    "            self.in_channels,\n",
    "            self.channels,\n",
    "            3,\n",
    "            padding=1,\n",
    "            conv_cfg=self.conv_cfg,\n",
    "            norm_cfg=self.norm_cfg,\n",
    "            act_cfg=self.act_cfg)\n",
    "        self.cam = CAM()\n",
    "        self.cam_out_conv = ConvModule(\n",
    "            self.channels,\n",
    "            self.channels,\n",
    "            3,\n",
    "            padding=1,\n",
    "            conv_cfg=self.conv_cfg,\n",
    "            norm_cfg=self.norm_cfg,\n",
    "            act_cfg=self.act_cfg)\n",
    "        self.cam_conv_seg = nn.Conv2d(\n",
    "            self.channels, self.num_classes, kernel_size=1)\n",
    "\n",
    "    def pam_cls_seg(self, feat):\n",
    "        \"\"\"PAM feature classification.\"\"\"\n",
    "        if self.dropout is not None:\n",
    "            feat = self.dropout(feat)\n",
    "        output = self.pam_conv_seg(feat)\n",
    "        return output\n",
    "\n",
    "    def cam_cls_seg(self, feat):\n",
    "        \"\"\"CAM feature classification.\"\"\"\n",
    "        if self.dropout is not None:\n",
    "            feat = self.dropout(feat)\n",
    "        output = self.cam_conv_seg(feat)\n",
    "        return output\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Forward function.\"\"\"\n",
    "        x = self._transform_inputs(inputs)\n",
    "        pam_feat = self.pam_in_conv(x)\n",
    "        pam_feat = self.pam(pam_feat)\n",
    "        pam_feat = self.pam_out_conv(pam_feat)\n",
    "        pam_out = self.pam_cls_seg(pam_feat)\n",
    "\n",
    "        cam_feat = self.cam_in_conv(x)\n",
    "        cam_feat = self.cam(cam_feat)\n",
    "        cam_feat = self.cam_out_conv(cam_feat)\n",
    "        cam_out = self.cam_cls_seg(cam_feat)\n",
    "\n",
    "        feat_sum = pam_feat + cam_feat\n",
    "        pam_cam_out = self.cls_seg(feat_sum)\n",
    "\n",
    "        return pam_cam_out, pam_out, cam_out\n",
    "\n",
    "    def forward_test(self, inputs, img_metas, test_cfg):\n",
    "        \"\"\"Forward function for testing, only ``pam_cam`` is used.\"\"\"\n",
    "        return self.forward(inputs)[0]\n",
    "\n",
    "    def losses(self, seg_logit, seg_label):\n",
    "        \"\"\"Compute ``pam_cam``, ``pam``, ``cam`` loss.\"\"\"\n",
    "        pam_cam_seg_logit, pam_seg_logit, cam_seg_logit = seg_logit\n",
    "        loss = dict()\n",
    "        loss.update(\n",
    "            add_prefix(\n",
    "                super(DAHead, self).losses(pam_cam_seg_logit, seg_label),\n",
    "                'pam_cam'))\n",
    "        loss.update(\n",
    "            add_prefix(\n",
    "                super(DAHead, self).losses(pam_seg_logit, seg_label), 'pam'))\n",
    "        loss.update(\n",
    "            add_prefix(\n",
    "                super(DAHead, self).losses(cam_seg_logit, seg_label), 'cam'))\n",
    "        return loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
