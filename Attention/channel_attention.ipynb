{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Channel Attention实现方式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SENet中的实现\n",
    "- Attention map shape 为torch.Size([16, 64, 1, 1])，相当于给每个通道指定了一个权重\n",
    "- https://github.com/moskomule/senet.pytorch/blob/master/senet/se_module.py\n",
    "- 其作用是去挖掘特征图通道之间的关系，去对通道之间的相互依赖关系，相关性进行显式的建模，增强模型的表达能力。\n",
    "- 从而学习利用全局信息去增强有用的特征，抑制作用少的特征。\n",
    "- In this paper, we investigate a different aspect of network\n",
    "design—the relationship between channels. We introduce a\n",
    "new architectural unit, which we term the Squeeze-and-Excitation\n",
    "(SE) block, with the goal of improving the quality of\n",
    "representations produced by a network by explicitly modelling\n",
    "the interdependencies between the channels of its convolutional\n",
    "features. To this end, we propose a mechanism\n",
    "that allows the network to perform feature recalibration,\n",
    "through which it can learn to use global information to\n",
    "selectively emphasise informative features and suppress\n",
    "less useful ones.\n",
    "\n",
    "In this paper we proposed the SE block, an architectural unit\n",
    "designed to improve the representational power of a network\n",
    "by enabling it to perform dynamic channel-wise feature recalibration.\n",
    "A wide range of experiments show the effectiveness\n",
    "of SENets, which achieve state-of-the-art performance across\n",
    "multiple datasets and tasks. In addition, SE blocks shed some\n",
    "light on the inability of previous architectures to adequately\n",
    "model channel-wise feature dependencies. We hope this\n",
    "insight may prove useful for other tasks requiring strong discriminative\n",
    "features.\n",
    "\n",
    "在这篇文章中我们提出了SE block，一个可以提高模型表达能力的结构单元，它是通过执行动态的通道特征对齐的方式来提高表达能力的。\n",
    "SE block为以前的模型提供充分建模通道间特征依赖指明了道路。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        print(b, c)\n",
    "        y = self.avg_pool(x).view(b, c) #squeeze produces a channel descriptor by aggregating feature maps across their spatial dimensions\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        print('attention map shape:', y.shape) #torch.Size([16, 64, 1, 1])\n",
    "        return x * y.expand_as(x) # *号表示哈达玛积，既element-wise乘积, 用输入x乘以attention map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 测试SENet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 64\n",
      "attention map shape: torch.Size([16, 64, 1, 1])\n",
      "torch.Size([16, 64, 64, 48])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "features = torch.randn(16, 64, 64,48)\n",
    "\n",
    "selayer = SELayer(64)\n",
    "\n",
    "feature_out = selayer(features) #通过SElayer之后，features的shape还是保持不变，torch.Size([16, 64, 64, 48])\n",
    "print(feature_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 上面SELyaer的如何嵌入在resdial block中\n",
    "- 基本是按照论文中standard SE block的设计来的\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SEBasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None,\n",
    "                 *, reduction=16):\n",
    "        super(SEBasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.se = SELayer(planes, reduction)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.se(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DANet中的实现\n",
    "- attention map shape 为torch.Size([16, 64, 64])，相当于两两通道之间的关系使用一个矩阵表示了出来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmcv.cnn import ConvModule, Scale\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CAM(nn.Module):\n",
    "    \"\"\"Channel Attention Module (CAM)\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CAM, self).__init__()\n",
    "        self.gamma = Scale(0) #A learnable scale parameter. 论文中的beta\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward function.\"\"\"\n",
    "        batch_size, channels, height, width = x.size()\n",
    "        proj_query = x.view(batch_size, channels, -1)\n",
    "        proj_key = x.view(batch_size, channels, -1).permute(0, 2, 1)\n",
    "        energy = torch.bmm(proj_query, proj_key) #matrix multiplication\n",
    "        print('energy shape: ', energy.shape)\n",
    "        energy_new = torch.max(\n",
    "            energy, -1, keepdim=True)[0].expand_as(energy) - energy\n",
    "        print('energy_new shape: ', energy_new.shape)\n",
    "        \n",
    "        attention = F.softmax(energy_new, dim=-1)\n",
    "        print('attention map shape: ', attention.shape) # torch.Size([16, 64, 64])\n",
    "        proj_value = x.view(batch_size, channels, -1)\n",
    "        print('proj_value shape:', proj_value.shape)\n",
    "        out = torch.bmm(attention, proj_value)\n",
    "        print('out shape: ',out.shape)\n",
    "        out = out.view(batch_size, channels, height, width)\n",
    "\n",
    "        out = self.gamma(out) + x\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "energy shape:  torch.Size([16, 64, 64])\n",
      "energy_new shape:  torch.Size([16, 64, 64])\n",
      "attention map shape:  torch.Size([16, 64, 64])\n",
      "proj_value shape: torch.Size([16, 64, 3072])\n",
      "out shape:  torch.Size([16, 64, 3072])\n",
      "torch.Size([16, 64, 64, 48])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "features = torch.randn(16, 64, 64,48)\n",
    "\n",
    "camlayer = CAM()\n",
    "\n",
    "feature_out = camlayer(features)\n",
    "print(feature_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ENCAM中的实现\n",
    "- 其与SENet的不同之处在于：SENet中的squeeze操作使用global average pooling来实现，pytorch函数为AdaptiveAvgPool2d，而ENCAM中使用global average pooling 和 global max pooling的和来实现，pytorch的函数为AdaptiveAvgPool2d + AdaptiveMaxPool2d\n",
    "- Attention map shape 为torch.Size([16, 64, 1, 1])，相当于给每个通道指定了一个权重\n",
    "\n",
    "- ENCAM中的映射使用的是2d卷积，而SENet中使用的是Linear层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes, ):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.fc1 = nn.Conv2d(in_planes, in_planes // 8, 1, bias=False)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Conv2d(in_planes // 8, in_planes, 1, bias=False)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n",
    "        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmoid(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention map shape:  torch.Size([16, 64, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "features = torch.randn(16, 64, 64,48)\n",
    "\n",
    "channel_attention = ChannelAttention(64)\n",
    "feature_out = channel_attention(features)\n",
    "print('attention map shape: ', feature_out.shape)\n",
    "\n",
    "#得到feature map之后使用原来的features*feature_out，相当于对原来的特征进行了通道维的增强。\n",
    "可以参考原论文中的Fig2. The architecture of the channel attention model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D squeeze and exitation \n",
    "https://github.com/ai-med/squeeze_and_excitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class SELayer3D(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SELayer3D, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool3d(1) #这个操作会将最后三个轴的维度变为1维\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _,_ = x.size()\n",
    "        print(b, c)\n",
    "        y = self.avg_pool(x).view(b, c) #squeeze produces a channel descriptor by aggregating feature maps across their spatial dimensions\n",
    "        y = self.fc(y).view(b, c, 1, 1, 1)\n",
    "        print('attention map shape:', y.shape) #torch.Size([16, 64, 1, 1, 1])\n",
    "        return x * y.expand_as(x) # *号表示哈达玛积，既element-wise乘积, 用输入x乘以attention map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 测试SELayer3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 64\n",
      "attention map shape: torch.Size([16, 64, 1, 1, 1])\n",
      "torch.Size([16, 64, 64, 48, 32])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "features = torch.randn(16, 64, 64,48, 32)\n",
    "\n",
    "selayer = SELayer3D(64)\n",
    "\n",
    "feature_out = selayer(features) #通过SElayer之后，features的shape还是保持不变torch.Size([16, 64, 64, 48, 32])\n",
    "print(feature_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 定义新的3d attention，使得其只关注原来通道维，而不影响band维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SELayer3DNew(nn.Module):\n",
    "    def __init__(self, channel, reduction=2):\n",
    "        super(SELayer3DNew, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool3d((None, 1, 1))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel//4 , bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(channel//4, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, band_num, _,_ = x.size()\n",
    "        #print(b, c)\n",
    "        y = self.avg_pool(x).view(b, c, band_num) #squeeze produces a channel descriptor by aggregating feature maps across their spatial dimensions\n",
    "        #print(y.shape)\n",
    "        y = self.fc(y).view(b, c, band_num, 1, 1)\n",
    "        #print('attention map shape:', y.shape) #torch.Size([16, 64, 1, 1, 1])\n",
    "        return x * y.expand_as(x) # *号表示哈达玛积，既element-wise乘积, 用输入x乘以attention map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 测试新的3D注意力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 64, 64, 48, 32])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "features = torch.randn(16, 64, 64,48, 32)\n",
    "\n",
    "selayer = SELayer3DNew(64)\n",
    "\n",
    "feature_out = selayer(features) #通过SElayer之后，features的shape还是保持不变torch.Size([16, 64, 64, 48, 32])\n",
    "print(feature_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ECA-Net\n",
    "- 相对于se-net改进的地方\n",
    "- Our ECA module aims at capturing local cross-channel\n",
    "interaction, which shares some similarities with channel local\n",
    "convolutions [35] and channel-wise convolutions [8];\n",
    "different from them, our method investigates a 1D convolution\n",
    "with adaptive kernel size to replace FC layers in channel\n",
    "attention module.\n",
    "- While dimensionality reduction in SE-Net can reduce\n",
    "model complexity, it destroys the direct correspondence between\n",
    "channel and its weight.For example, one single FC\n",
    "layer predicts weight of each channel using a linear combination\n",
    "of all channels，But Eq. (2) first projects channel\n",
    "features into a low-dimensional space and then maps\n",
    "them back, making correspondence between channel and\n",
    "its weight be indirect.(虽然降维可以在SE-Net中降低模型的复杂度，但是他也同时破坏了通道和其权重之间的直接相关性。比如只使用一个全连接层就可以通过所有通道的线性组合去预测某一个通道的权重，但是SE-Net中首先将通道特征映射到低维空间中，再将他们映射回来，反而使得通道和其权重之间变成了间接相关。)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class eca_layer(nn.Module):\n",
    "    \"\"\"Constructs a ECA module.\n",
    "    Args:\n",
    "        channel: Number of channels of the input feature map\n",
    "        k_size: Adaptive selection of kernel size\n",
    "    \"\"\"\n",
    "    def __init__(self, channel, k_size=3):\n",
    "        super(eca_layer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.conv = nn.Conv1d(1, 1, kernel_size=k_size, padding=(k_size - 1) // 2, bias=False) \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: input features with shape [b, c, h, w]\n",
    "        b, c, h, w = x.size()\n",
    "        print(b, c, h, w)\n",
    "        # feature descriptor on the global spatial information\n",
    "        y = self.avg_pool(x) #将最后两维压缩掉了\n",
    "\n",
    "        print('y.shape =', y.shape) #(b, c, 1, 1)\n",
    "        print('y.squeeze = ',y.squeeze(-1).shape)#y.squeeze(-1)表示压缩掉最后一维\n",
    "        print('transpose =', y.squeeze(-1).transpose(-1, -2).shape) #transpose(-1, -2)表示交换倒数第一维和倒数第二维\n",
    "        print('atter conv1d shape =', self.conv(y.squeeze(-1).transpose(-1, -2)).shape)\n",
    "        print('after transpose shape =', self.conv(y.squeeze(-1).transpose(-1, -2)).transpose(-1, -2).shape)\n",
    "        # Two different branches of ECA module\n",
    "        y = self.conv(y.squeeze(-1).transpose(-1, -2)).transpose(-1, -2).unsqueeze(-1) #unsqueeze(-1) 表示在最后一维增加一个维度\n",
    "        print('after y shape =', y.shape)\n",
    "        # Multi-scale information fusion\n",
    "        y = self.sigmoid(y)\n",
    "\n",
    "        return x * y.expand_as(x) #最后使用获得的注意力对原始输入进行了加权"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ECA 测试代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 32 64 64\n",
      "y.shape = torch.Size([1, 32, 1, 1])\n",
      "y.squeeze =  torch.Size([1, 32, 1])\n",
      "transpose = torch.Size([1, 1, 32])\n",
      "atter conv1d shape = torch.Size([1, 1, 32])\n",
      "after transpose shape = torch.Size([1, 32, 1])\n",
      "after y shape = torch.Size([1, 32, 1, 1])\n",
      "torch.Size([1, 32, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 32, 64, 64)\n",
    "\n",
    "efficient_channel_attention = eca_layer(32)\n",
    "\n",
    "output = efficient_channel_attention(input)\n",
    "\n",
    "print(output.shape) #torch.Size([1, 32, 64, 64]),也就是输出的shape与输入的shape是相同的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ECA模块在residual Net中的应用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "class ECABasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, k_size=3):\n",
    "        super(ECABasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.eca = eca_layer(planes, k_size)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.eca(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "        \n",
    "        #print('out.shape: ', out.shape)\n",
    "        #print('residual.shape: ', residual.shape)\n",
    "        out += residual #将注意力加权之后的结果，加上了原来的residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 测试ECABasicBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 64, 64])\n",
      "torch.Size([1, 16, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 32, 64, 64)\n",
    "\n",
    "eca_block = ECABasicBlock(32,32)\n",
    "\n",
    "output = eca_block(input)\n",
    "print(output.shape) #torch.Size([1, 32, 64, 64])\n",
    "\n",
    "\n",
    "down_sample = conv3x3(32, 16)\n",
    "eca_block_with_down_smaple = ECABasicBlock(32,16, 1, down_sample)\n",
    "\n",
    "output = eca_block_with_down_smaple(input)\n",
    "print(output.shape) #torch.Size([1, 16, 64, 64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义基于ECA的3D卷积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class eca_layer3d(nn.Module):\n",
    "    \"\"\"Constructs a ECA module.\n",
    "    Args:\n",
    "        channel: Number of channels of the input feature map\n",
    "        k_size: Adaptive selection of kernel size\n",
    "    \"\"\"\n",
    "    def __init__(self, channel, k_size=3):\n",
    "        super(eca_layer3d, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool3d((None, 1, 1))\n",
    "        self.conv = nn.Conv1d(channel, channel, kernel_size=k_size, padding=(k_size - 1) // 2, bias=False) \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: input features with shape [b, c, h, w]\n",
    "        b, c, depth, h, w = x.size()\n",
    "        print(b, c, depth, h, w)\n",
    "        # feature descriptor on the global spatial information\n",
    "        y = self.avg_pool(x) #将最后两维压缩掉了\n",
    "\n",
    "        print('y.shape =', y.shape) #(b, c, 1, 1)\n",
    "        print('y.squeeze = ',y.squeeze(-1).shape)#y.squeeze(-1)表示压缩掉最后一维\n",
    "        #print('transpose =', y.squeeze(-1).transpose(-1, -2).shape) #transpose(-1, -2)表示交换倒数第一维和倒数第二维\n",
    "        #print('atter conv1d shape =', self.conv(y.squeeze(-1).transpose(-1, -2)).shape)\n",
    "        #print('after transpose shape =', self.conv(y.squeeze(-1).transpose(-1, -2)).transpose(-1, -2).shape)\n",
    "        print('y.squeeze.squeeze shape =', y.squeeze(-1).squeeze(-1).shape)\n",
    "        # Two different branches of ECA module\n",
    "        y = self.conv(y.squeeze(-1).squeeze(-1)).unsqueeze(-1).unsqueeze(-1) #unsqueeze(-1) 表示在最后一维增加一个维度\n",
    "        print('after y shape =', y.shape)\n",
    "        # Multi-scale information fusion\n",
    "        y = self.sigmoid(y)\n",
    "\n",
    "        return x * y.expand_as(x) #最后使用获得的注意力对原始输入进行了加权"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 测试基于eca的3d注意力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 64 64 48 32\n",
      "y.shape = torch.Size([16, 64, 64, 1, 1])\n",
      "y.squeeze =  torch.Size([16, 64, 64, 1])\n",
      "y.squeeze.squeeze shape = torch.Size([16, 64, 64])\n",
      "after y shape = torch.Size([16, 64, 64, 1, 1])\n",
      "torch.Size([16, 64, 64, 48, 32])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "features = torch.randn(16, 64, 64,48, 32)\n",
    "\n",
    "selayer = eca_layer3d(64)\n",
    "\n",
    "feature_out = selayer(features) #通过SElayer之后，features的shape还是保持不变torch.Size([16, 64, 64, 48, 32])\n",
    "print(feature_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 兼顾channel维和depth维的3d注意力\n",
    "\n",
    "- 先计算最后两维的注意力，再计算最后三维的注意力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class eca_depth_spatial_layer3d(nn.Module):\n",
    "    \"\"\"Constructs a ECA module.\n",
    "    Args:\n",
    "        channel: Number of channels of the input feature map\n",
    "        k_size: Adaptive selection of kernel size\n",
    "    \"\"\"\n",
    "    def __init__(self, channel, k_size=3):\n",
    "        super(eca_depth_spatial_layer3d, self).__init__()\n",
    "        self.avg_pool_spatial = nn.AdaptiveAvgPool3d((None, 1, 1))\n",
    "        self.avg_pool_depth = nn.AdaptiveAvgPool3d(1)\n",
    "        self.conv_spatial = nn.Conv1d(channel, channel, kernel_size=k_size, padding=(k_size - 1) // 2, bias=False) \n",
    "        self.conv_depth = nn.Conv1d(1, 1, kernel_size=k_size, padding=(k_size - 1) // 2, bias=False) \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: input features with shape [b, c, h, w]\n",
    "        b, c, depth, h, w = x.size()\n",
    "        print(b, c, depth, h, w)\n",
    "        # feature descriptor on the global spatial information\n",
    "        y = self.avg_pool_spatial(x) #将最后两维压缩掉了\n",
    "\n",
    "        print('y.shape =', y.shape) #(b, c, 1, 1)\n",
    "        print('y.squeeze = ',y.squeeze(-1).shape)#y.squeeze(-1)表示压缩掉最后一维\n",
    "        #print('transpose =', y.squeeze(-1).transpose(-1, -2).shape) #transpose(-1, -2)表示交换倒数第一维和倒数第二维\n",
    "        #print('atter conv1d shape =', self.conv(y.squeeze(-1).transpose(-1, -2)).shape)\n",
    "        #print('after transpose shape =', self.conv(y.squeeze(-1).transpose(-1, -2)).transpose(-1, -2).shape)\n",
    "        print('y.squeeze.squeeze shape =', y.squeeze(-1).squeeze(-1).shape)\n",
    "        # Two different branches of ECA module\n",
    "        spatial_y = self.conv_spatial(y.squeeze(-1).squeeze(-1)).unsqueeze(-1).unsqueeze(-1) #unsqueeze(-1) 表示在最后一维增加一个维度\n",
    "        print('after y shape =', spatial_y.shape)\n",
    "        # Multi-scale information fusion\n",
    "        spatial_atten = self.sigmoid(spatial_y)\n",
    "\n",
    "        depth_y = self.avg_pool_depth(x)\n",
    "        \n",
    "        depth_y = self.conv_depth(depth_y.squeeze(-1).squeeze(-1).transpose(-1, -2)).transpose(-1, -2).unsqueeze(-1).unsqueeze(-1)\n",
    "        depth_atten = self.sigmoid(depth_y)\n",
    "        \n",
    "        return (x * spatial_atten.expand_as(x)) * depth_atten.expand_as(x) #最后使用获得的注意力对原始输入进行了加权"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- eca_depth_spatial_layer3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 64 64 48 32\n",
      "y.shape = torch.Size([16, 64, 64, 1, 1])\n",
      "y.squeeze =  torch.Size([16, 64, 64, 1])\n",
      "y.squeeze.squeeze shape = torch.Size([16, 64, 64])\n",
      "after y shape = torch.Size([16, 64, 64, 1, 1])\n",
      "torch.Size([16, 64, 64, 48, 32])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "features = torch.randn(16, 64, 64,48, 32)\n",
    "\n",
    "selayer = eca_depth_spatial_layer3d(64)\n",
    "\n",
    "feature_out = selayer(features) #通过SElayer之后，features的shape还是保持不变torch.Size([16, 64, 64, 48, 32])\n",
    "print(feature_out.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
