{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文中介绍SAGAN中的attention\n",
    "\n",
    "- 来自于https://github.com/heykeetae/Self-Attention-GAN\n",
    "- https://zhuanlan.zhihu.com/p/110130098"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 源码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from spectral import SpectralNorm\n",
    "import numpy as np\n",
    "\n",
    "class Self_Attn(nn.Module):\n",
    "    \"\"\" Self attention Layer\"\"\"\n",
    "    def __init__(self,in_dim,activation):\n",
    "        super(Self_Attn,self).__init__()\n",
    "        self.chanel_in = in_dim\n",
    "        self.activation = activation\n",
    "        self.query_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//8 , kernel_size= 1)\n",
    "        self.key_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//8 , kernel_size= 1)\n",
    "        self.value_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim , kernel_size= 1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "        self.softmax  = nn.Softmax(dim=-1) #\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "            inputs :\n",
    "                x : input feature maps( B X C X W X H)\n",
    "            returns :\n",
    "                out : self attention value + input feature \n",
    "                attention: B X N X N (N is Width*Height)\n",
    "        \"\"\"\n",
    "        m_batchsize,C,width ,height = x.size()\n",
    "        \n",
    "        # B X C X (N) permute(0, 2, 1)相当于对矩阵进行了转置\n",
    "        proj_query  = self.query_conv(x).view(m_batchsize,-1,width*height).permute(0,2,1) \n",
    "        proj_key =  self.key_conv(x).view(m_batchsize,-1,width*height) # B X C x (*W*H)\n",
    "        \n",
    "        energy =  torch.bmm(proj_query,proj_key) # transpose check\n",
    "        attention = self.softmax(energy) \n",
    "        # B X (N) X (N) #计算了一张图中每个像素点与其它像素点的关系，\n",
    "        #对于一个像素点，其与其它N个像素点的关系都可以得到\n",
    "        #将N展开成WxH，可以得到，该像素点的对其它所有特征点的attention map\n",
    "        \n",
    "        proj_value = self.value_conv(x).view(m_batchsize,-1,width*height) # B X C X N\n",
    "\n",
    "        #bmm中的b表示的是batch ，将attention施加到特征图上，得到最终的特征图\n",
    "        out = torch.bmm(proj_value,attention.permute(0,2,1) ) \n",
    "        out = out.view(m_batchsize,C,width,height)\n",
    "        out = self.gamma*out + x\n",
    "        return out,attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 源码解释\n",
    "\n",
    "- 来自于https://zhuanlan.zhihu.com/p/110130098\n",
    "\n",
    "在forward函数中，定义了self-attention的具体步骤。\n",
    "\n",
    "步骤一：\n",
    "\n",
    "proj_query  = self.query_conv(x).view(m_batchsize,-1,width*height).permute(0,2,1)\n",
    "proj_query本质上就是卷积，只不过加入了reshape的操作。首先是对输入的feature map进行query_conv卷积，输出为B×C/8×W×H；view函数是改变了输出的维度，就单张feature map而言，就是将W×H大小拉直，变为1×(W×H)大小；就batchsize大小而言，输出就是B×C/8×(W×H)；permute函数则对第二维和第三维进行倒置，输出为B×(W×H)×C/8。proj_query中的第i行表示第i个像素位置上所有通道的值。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "proj_key =  self.key_conv(x).view(m_batchsize,-1,width*height)\n",
    "proj_key与proj_query相似，只是没有最后一步倒置，输出为B×C/8×(W×H)。proj_key中的第j行表示第j个像素位置上所有通道的值。\n",
    "\n",
    "\n",
    "步骤二：\n",
    "\n",
    "energy =  torch.bmm(proj_query,proj_key)\n",
    "这一步是将batch_size中的每一对proj_query和proj_key分别进行矩阵相乘，输出为B×(W×H)×(W×H)。Energy中的第(i,j)是将proj_query中的第i行与proj_key中的第j行点乘得到。这个步骤的意义是energy中第(i,j)位置的元素是指输入特征图第j个元素对第i个元素的影响，从而实现全局上下文任意两个元素的依赖关系。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "步骤三：\n",
    "\n",
    "attention = self.softmax(energy)\n",
    "这一步是将energe进行softmax归一化，是对行的归一化。归一化后每行的之和为1，对于(i,j)位置即可理解为第j位置对i位置的权重，所有的j对i位置的权重之和为1，此时得到attention_map。\n",
    "\n",
    "proj_value = self.value_conv(x).view(m_batchsize,-1,width*height)\n",
    "proj_value和proj_query与proj_key一样，只是输入为B×C×W×H，输出为B×C×(W×H)。从self-attention结构图中可以知道proj_value是与attention_map进行矩阵相乘，即下面两行代码。\n",
    "\n",
    "out = torch.bmm(proj_value,attention.permute(0,2,1) )\n",
    "out = out.view(m_batchsize,C,width,height)\n",
    "在对proj_value与attention_map点乘之前，先对attention进行转置。这是由于attention中每一行的权重之和为1，是原特征图第j个位置对第i个位置的权重，将其转置之后，每一列之和为1；proj_value的每一行与attention中的每一列点乘，将权重施加于proj_value上，输出为B×C×(W×H)。\n",
    "\n",
    "out = self.gamma*out + x\n",
    "这一步是对attention之后的out进行加权，x是原始的特征图，将其叠加在原始特征图上。Gamma是经过学习得到的，初始gamma为0，输出即原始特征图，随着学习的深入，在原始特征图上增加了加权的attention，得到特征图中任意两个位置的全局依赖关系。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## attention map的可视化\n",
    "\n",
    "- 在测试的时候，将上面代码中的attention保存下来即可可视化\n",
    "- We visualize the attention maps of the last generator layer that used attention, since this layer is the closest to the output pixels and is the most straightforward to project into pixel space and interpret.\n",
    "- SA GAN使用的是generator的最后一个使用attention的层来可视化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 疑问\n",
    "\n",
    "向量的similarity 与注意力有什么关系。\n",
    "如何理解paper中的这句话：\n",
    "where sji measures the ith position’s impact on jth position.\n",
    "The more similar feature representations of the two\n",
    "position contributes to greater correlation between them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
